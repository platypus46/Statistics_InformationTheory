{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정보이론(information theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정보이론은 '사건이 지닌 정보를 정량화할 수 있는가'에 의의를 갖는다.    \n",
    "정보이론에서는 기본적으로 확률이 작을수록 많은 정보를 갖음  \n",
    "**즉, 자주 일어나는 사건보다 잘 일어나지 않는 사건의 정보량이 많다고 할 수 있다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자기정보(self-information)\n",
    "\n",
    "다른 사건과는 관련없이 순전히 단일 사건 만의 발생 확률에 근거한 정보량을 말한다.  \n",
    "1. $h(e_i)=-log_2P(e_i)$\n",
    "2. $h(e_i)=-log_eP(e_i)$  \n",
    "보통 계산에 용이하게 하기 위해 $h(e_i)=-log_2P(e_i)$를 사용한다.\n",
    "<br/>  \n",
    "<예시>\n",
    "1. 동전이 앞면이 나오는 경우: $-log_2(1/2)=1$\n",
    "2. 주사위에서 1이 나오는 경우: $-log_2(1/6)\\fallingdotseq2.58$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 엔트로피(Entrophy)\n",
    "- 정보를 표현하는데 필요 최소 평균 자원량\n",
    "- 확률변수 x의 불확실성을 나타내는데 사용\n",
    "- 모든 정보를 기대값으로 표현\n",
    "\n",
    "**이산확률분포**  \n",
    "$H(x)=-\\displaystyle\\sum_{i=1,k}^{}{P(e_i)log_2P(e_i)}$\n",
    "\n",
    "**연속확률분포**  \n",
    "$H(x)=-\\int\\limits_{R}^{}{P(e_i)log_2P(e_i)} $\n",
    "\n",
    "<예시>  \n",
    "동전의 앞뒤 발생확률이 동일하다면?  \n",
    "$H(x)=-(0.5*log_20.5+0.5*log_20.5)=1$\n",
    "\n",
    "위의 내용을 바탕으로 모든 사건이 동일한 확률을 가질때 불확실성이 가장높으며 엔트로피가 최고임을 알 수 있음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 쿨백-라이블러 발산(kullback–leibler divergence, KLD)\n",
    "두 확률분포의 차이를 계산하는 데에 사용하는 함수이다.\n",
    "\n",
    "$KL(P||Q)=-\\displaystyle\\sum_{x}^{}P(x)log_2\\frac{P(x)}{Q(x)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 교차 엔트로피(cross entrophy)\n",
    "실제 분포와 예측 분포가 다르다는 특징을 이용하여 실제 분포를 가정한 후 예측한 분포에 따른 정보획득의 유용성을 나타낸것  \n",
    "$H(P,Q)=-\\displaystyle\\sum_{x}^{}P(x)log_2Q(x)$\n",
    "\n",
    "위의 식을 전개하면  \n",
    "$-\\displaystyle\\sum_{x}^{}P(x)log_2P(x)+\\displaystyle\\sum_{x}^{}P(x)log_2P(x)-\\displaystyle\\sum_{x}^{}P(x)log_2Q(x)$  \n",
    "=$H(P)+\\displaystyle\\sum_{x}^{}P(x)log_2\\frac{P(x)}{Q(x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "교차 엔트로피를 손실함수로 사용하는 경우, KL발산의 최소화함과 동일하다는 것을 알 수 있으며 \n",
    "\n",
    "따라서 이를 정리해본다면 가지고 있는 데이터 분포 P(x)와 추정한 데이터 분포Q(x)간의 차이를 최소화 할 때 교차엔트로피를 사용한다는 것을 알 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
